# cp .env.example .env
# Edit your .env file with your own values
# Don't commit your .env file to git/push to GitHub!
# Don't modify/delete .env.example unless adding extensions to the project
# which require new variable to be added to the .env file

# ----------------------------------
# API CONFIG
# OPENAI_API_MODEL can be used instead
# Special values:
# human - use human as intermediary with custom LLMs
# llama - use llama.cpp with Llama, Alpaca, Vicuna, GPT4All, etc.
# ----------------------------------
LLM_MODEL=gpt-3.5-turbo
MAX_TOKENS=2000   # OpenAI: 2000 / 7B-Llama: 1000 works best

# OPENAI API CONFIG
OPENAI_API_KEY=
OPENAI_API_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.5

# LLAMA model config (parameters enhanced for small local running Llama's)
LLAMA_TEMPERATURE=0.8   # default: 0.2 (values between 0.8 to 0.9 seem to work best for 7B-Llama)
LLAMA_CONTEXT=2000      # 7B-Llama: 2000 (different factors are applied on top for e.g. result storage context, document context, internet context, etc.)
LLAMA_CTX_MAX=1024      # default: 1024
LLAMA_THREADS_NUM=8     # default: 8
LLAMA_FAILSAFE=false    # default: false (experimental feature for re-iteration of process steps in case of context loss)
LLAMA_MODEL_PATH=

# ----------------------------------
# STORE CONFIG
# TABLE_NAME can be used instead
# ----------------------------------
RESULTS_STORE_NAME=babyagi-db  # for chromadb directory 'chroma' is used instead

# Pinecone config
# Uncomment and fill these to switch from local ChromaDB to Pinecone
# PINECONE_API_KEY=
# PINECONE_ENVIRONMENT=

# Weaviate config
# Uncomment and fill these to switch from local ChromaDB to Weaviate
# WEAVIATE_USE_EMBEDDED=true
# WEAVIATE_URL=
# WEAVIATE_API_KEY=

# ----------------------------------
# COOPERATIVE MODE CONFIG
# BABY_NAME can be used instead
# ----------------------------------
INSTANCE_NAME=BabyAGI
COOPERATIVE_MODE=none

# RUN CONFIG
OBJECTIVE=Solve world hunger.

#Research the history of Madrid, from its founding to the present day. Create a timeline of the most important events in the history of Madrid.

# For backwards compatibility
# FIRST_TASK can be used instead of INITIAL_TASK
INITIAL_TASK=Develop a task list

# Extensions
# List additional extension .env files to load (except .env.example!)
DOTENV_EXTENSIONS=

# Set to true to enable command line args support
ENABLE_COMMAND_LINE_ARGS=false

# ----------------------------------
# Write output to file (Experimental)
# Intended for step-by-step creation of a summary, story, report, etc. for the objective
# TODO: Add report parsing, processing, update and summarization functionality
# ----------------------------------
ENABLE_REPORT_EXTENSION=false
REPORT_FILE=report.txt

# Action to be performed for the OBJECTIVE
# The description 'chapter by chapter' and 'create a draft' must be kept, the rest can be adapted
ACTION=Develop a Short Story, chapter by chapter, which is in line with the Warhammer40k universe, and create a draft about 2000 words.

# Instruction for text output to report file
INSTRUCTION=Begin and end output with text for the draft with marking 'Report-Tag'.
REPORT_TITLE=Short Story

# ----------------------------------
# Internet smart search extension (based on smart search from BabyCatAGI)
# SERPAPI, Google CSE or browser search possible (works also w/o any API key!)
# Fallback strategy for missing API key and API rate limit
# Summarization of web scraping results with OpenAI or Llama
# ----------------------------------
ENABLE_SEARCH_EXTENSION=true
GOOGLE_API_KEY=
GOOGLE_CSE_ID=
SERPAPI_API_KEY=

# Temperature and CTX value for embedding LLM
SUMMARY_TEMPERATURE=0.7     # OpenAI: 0.7
SUMMARY_CTX_MAX=1024        # 7B-Llama: 1024 (used for Llama only, adapt SUMMARY_CONTEXT below accordingly)

# Context and webpage scrape length limits for smart internet search
SUMMARY_CONTEXT=3000   # OpenAI: 3000 (value of 1000 works fine or 7B-Llama)
SCRAPE_LENGTH=5000     # OpenAI: 5000 (value of 3000 is a good trade-off for 7B-Llama)

# Search summary model path (default: leave empty for gpt-3.5-turbo)
SUMMARY_MODEL_PATH=

# ----------------------------------
# Document embedding extension using langchain and chromadb
# The functionality is from: https://github.com/imartinez/privateGPT.git
# Relevant content from file privateGPT.py has integrated in BabyAGI
# The file ingest.py has been slightly adapted and renamed to document-loader.py
# Many thanks to https://github.com/imartinez for the great work!
# ----------------------------------
ENABLE_DOCUMENT_EXTENSION=true
ENABLE_DOC_UPDATE=true  # default: false (Enable for update of document embeddings storeage with internet scraping results)
DOC_STORE_NAME=chroma-warhammer40k
DOC_SOURCE_PATH=source_documents

# Document embedding model config
EMBEDDINGS_TEMPERATURE=0.8      # default: 0.8 (LlamaCpp) or 0.7 (GPT4All)
EMBEDDINGS_MODEL_TYPE=LlamaCpp  # LlamaCpp or GPT4All
EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
EMBEDDINGS_MODEL_PATH=

