# cp .env.example .env
# Edit your .env file with your own values
# Don't commit your .env file to git/push to GitHub!
# Don't modify/delete .env.example unless adding extensions to the project
# which require new variable to be added to the .env file

# ----------------------------------
# API CONFIG
# OPENAI_API_MODEL can be used instead
# Special values:
# human - use human as intermediary with custom LLMs
# llama - use llama.cpp with Llama, Alpaca, Vicuna, GPT4All, etc.
# ----------------------------------
LLM_MODEL=gpt-3.5-turbo

# OPENAI API CONFIG
OPENAI_API_KEY=
OPENAI_API_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.5

# LLAMA model config (parameters enhanced for small local running Llama's)
LLAMA_TEMPERATURE=0.9   # 7B-Llama: 0.9
LLAMA_CONTEXT=1000      # 7B-Llama: 1000
LLAMA_CTX_MAX=1024      # 7B-Llama: 1024
LLAMA_THREADS_NUM=8     # default: 8
LLAMA_FAILSAFE=false    # Experimental: Enable failsafe mechanisms as protection against context loss (designed for 7B-Llama with very limited context size)
LLAMA_MODEL_PATH=

# ----------------------------------
# STORE CONFIG
# TABLE_NAME can be used instead
# ----------------------------------
RESULTS_STORE_NAME=chat-db
PERSISTENT_STORAGE=true     # default: true

# Pinecone config
# Uncomment and fill these to switch from local ChromaDB to Pinecone
# PINECONE_API_KEY=
# PINECONE_ENVIRONMENT=

# Weaviate config
# Uncomment and fill these to switch from local ChromaDB to Weaviate
# WEAVIATE_USE_EMBEDDED=true
# WEAVIATE_URL=
# WEAVIATE_API_KEY=

# ----------------------------------
# COOPERATIVE MODE CONFIG
# BABY_NAME can be used instead
# ----------------------------------
INSTANCE_NAME=BabyAGI
COOPERATIVE_MODE=none

# RUN CONFIG
OBJECTIVE=Solve world hunger.
# For backwards compatibility
# FIRST_TASK can be used instead of INITIAL_TASK
INITIAL_TASK=Develop a task list

# Extensions
# List additional extension .env files to load (except .env.example!)
DOTENV_EXTENSIONS=

# Set to true to enable command line args support
ENABLE_COMMAND_LINE_ARGS=false

# ----------------------------------
# Internet smart search extension (based on smart search from BabyCatAGI)
# SERPAPI, Google CSE or browser search possible (works also w/o any API key!)
# Fallback strategy for missing API key and API rate limit
# Summarization of web scraping results with OpenAI or Llama
# ----------------------------------
ENABLE_SEARCH_EXTENSION=true
#GOOGLE_API_KEY=
#GOOGLE_CSE_ID=
#SERPAPI_API_KEY=

# Context and webpage scrape length limits for smart internet search
SUMMARY_CONTEXT=2000   # OpenAI: 4000 (value of 2000 works for 7B-Llama)
SCRAPE_LENGTH=3000     # OpenAI: 5000 (value of 3000 works for 7B-Llama)

# Temperature and CTX value for embedding LLM
SUMMARY_TEMPERATURE=0.7     # OpenAI: 0.7
SUMMARY_CTX_MAX=1024        # 7B-Llama: 1024 (used for Llama only)

# Search summary model path (default: leave empty for gpt-3.5-turbo)
SUMMARY_MODEL_PATH=

# ----------------------------------
# Document embedding extension using langchain and chromadb
# The functionality is from: https://github.com/imartinez/privateGPT.git
# Relevant content from file privateGPT.py has integrated in BabyAGI
# The file ingest.py has been slightly adapted and renamed to document-loader.py
# Many thanks to https://github.com/imartinez for the great work!
# ----------------------------------
ENABLE_DOCUMENT_EXTENSION=false
DOC_STORE_NAME=chroma-documents

# Document embedding model config
EMBEDDINGS_TEMPERATURE=0.8      # default: 0.8 (LlamaCpp) or 0.7 (GPT4All)
EMBEDDINGS_MODEL_TYPE=LlamaCpp  # LlamaCpp or GPT4All
EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
EMBEDDINGS_MODEL_PATH=

